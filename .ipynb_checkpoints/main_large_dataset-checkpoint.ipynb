{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries\n",
    "\n",
    "This makes use of tflearn and tensorflow. Generally speaking, better results were used when using higher level wrappers such as tflearn or slim, so we elected to use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.recurrent import bidirectional_rnn, BasicLSTMCell\n",
    "from tflearn.layers.core import dropout\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from audio_dataset_generator import AudioDatasetGenerator\n",
    "import numpy as np\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Program Parameters\n",
    "\n",
    "Set the appropriate settings for the model here. Audio data path will choose which folder to get fft frames from. Make sure the files are .wav and they are the only files you want as all the files fft frames are loaded and concatenated together. \n",
    "\n",
    "Please note:\n",
    "* ```rnn_type``` should equal any of the following values: lstm, gru, bi_lstm, bi_gru\n",
    "* ```number_rnn_layers``` should be greater than 0\n",
    "* ```activation``` should be the string defined in the tflearn library for [any of the activations defined here](http://tflearn.org/activations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "sequence_length      = 48\n",
    "audio_data_path      = \"assets/test_samples\"\n",
    "force_new_dataset    = True\n",
    "\n",
    "# Feature Extraction and Audio Genreation\n",
    "sample_rate          = 11025\n",
    "fft_settings         = [1024, None, None]\n",
    "fft_size             = fft_settings[0]\n",
    "window_size          = fft_settings[1]\n",
    "hop_size             = fft_settings[2]\n",
    "\n",
    "# General Network\n",
    "learning_rate        = 0.0004\n",
    "amount_epochs        = 1200\n",
    "batch_size           = 32\n",
    "dropout              = 0.0\n",
    "loss_type            = \"mean_square\"\n",
    "activation           = 'elu'\n",
    "optimiser            = 'adam'\n",
    "fully_connected_dim  = int(fft_size / 2 + 1)\n",
    "\n",
    "# Recurrent Neural Network\n",
    "rnn_type             = \"lstm\"\n",
    "number_rnn_layers    = 4\n",
    "rnn_number_units     = 128\n",
    "\n",
    "# Convolutional Neural Network\n",
    "use_cnn              = False\n",
    "number_filters       = [32, 64]\n",
    "filter_sizes         = [3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load the Dataset\n",
    "\n",
    "Take the fft magnitudes from the folder specified at the audio_data_path and create the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = AudioDatasetGenerator(fft_size, window_size, hop_size,\n",
    "                                sequence_length, sample_rate)\n",
    "\n",
    "dataset.load(audio_data_path, force_new_dataset)\n",
    "\n",
    "if use_cnn:\n",
    "    dataset.x_frames = dataset.x_frames.reshape(dataset.x_frames.shape[0], \n",
    "                                                dataset.x_frames.shape[1], \n",
    "                                                dataset.x_frames.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "A couple of helper methods are defined to speed up the process of experimenting with the model's archetecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(net, filters, kernels, non_linearity):  \n",
    "    \"\"\"\n",
    "    A quick function to build a conv net. \n",
    "    At the end it reshapes the network to be 3d to work with recurrent units.\n",
    "    \"\"\"\n",
    "    assert len(filters) == len(kernels)\n",
    "    \n",
    "    for i in range(len(filters)):\n",
    "        net = conv_2d(net, filters[i], kernels[i], activation=non_linearity)\n",
    "        net = max_pool_2d(net, 2)\n",
    "        \n",
    "    dim1 = net.get_shape().as_list()[1]\n",
    "    dim2 = net.get_shape().as_list()[2]\n",
    "    dim3 = net.get_shape().as_list()[3]\n",
    "    return tf.reshape(net, [-1, dim1 * dim3, dim2])\n",
    "   \n",
    "                      \n",
    "def recurrent_net(net, rec_type, rec_size, return_sequence):\n",
    "    \"\"\"\n",
    "    A quick if else block to build a recurrent layer, based on the type specified\n",
    "    by the user.\n",
    "    \"\"\"\n",
    "    if rec_type == 'lstm':\n",
    "        net = tflearn.layers.recurrent.lstm(net, rec_size, return_seq=return_sequence)\n",
    "    elif rec_type == 'gru':\n",
    "        net = tflearn.layers.recurrent.gru(net, rec_size, return_seq=return_sequence)\n",
    "    elif rec_type == 'bi_lstm':\n",
    "        net = bidirectional_rnn(net, \n",
    "                                BasicLSTMCell(rec_size), \n",
    "                                BasicLSTMCell(rec_size), \n",
    "                                return_seq=return_sequence)\n",
    "    elif rec_type == 'bi_gru':\n",
    "        net = bidirectional_rnn(net, \n",
    "                                GRUCell(rec_size), \n",
    "                                GRUCell(rec_size), \n",
    "                                return_seq=return_sequence)\n",
    "    else:\n",
    "        raise ValueError('Incorrect rnn type passed. Try lstm, gru, bi_lstm or bi_gru.')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the actual structure of the net is specified.\n",
    "\n",
    "If use_cnn is true the model will be prefixed with a cnn. This will slow things down but produces a different kind of result. The model regardless then builds a rnn layer which runs into a fully connected layer. before being passed to linear outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if use_cnn:\n",
    "    net = tflearn.input_data([None, \n",
    "                              dataset.x_frames.shape[1], \n",
    "                              dataset.x_frames.shape[2], \n",
    "                              dataset.x_frames.shape[3]])\n",
    "    net = conv_net(net, number_filters, filter_sizes, activation)\n",
    "else:                  \n",
    "    net = tflearn.input_data([None, \n",
    "                              dataset.x_frames.shape[1], \n",
    "                              dataset.x_frames.shape[2]]) \n",
    "\n",
    "for layer in range(number_rnn_layers):\n",
    "    return_sequence = False if layer == (number_rnn_layers - 1) else True\n",
    "    net = recurrent_net(net, rnn_type, rnn_number_units, return_sequence)\n",
    "    net = dropout(net, dropout) if dropout > 0.0 else net\n",
    "    \n",
    "net = tflearn.fully_connected(net, fully_connected_dim, \n",
    "                              activation=activation,                                            \n",
    "                              regularizer='L2', \n",
    "                              weight_decay=0.001)\n",
    "                      \n",
    "net = tflearn.fully_connected(net, dataset.y_frames.shape[1], \n",
    "                              activation='linear')\n",
    "                      \n",
    "net = tflearn.regression(net, optimizer=optimiser, learning_rate=learning_rate,                                 \n",
    "                         loss=loss_type)\n",
    "\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Currently we aren't worried about overfitting, so we just pass the entire dataset of generated magnitudes. Perhaps we might want to change this, in which case we would need to split the dataset into training, validation and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(dataset.x_frames, dataset.y_frames, show_metric=True, \n",
    "          batch_size=batch_size, n_epoch=amount_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from email_notification import EmailNotification\n",
    "notifications_on = True\n",
    "email_address = 'wprim001@gold.ac.uk'\n",
    "email = EmailNotification(email_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Audio\n",
    "\n",
    "Here we generate audio. Choose the amount of samples, then how long they should be with sequence length, and then how many iterations the griffin lim algorithm should run for. The impluse scale is something we haven't objectively tested yet, but it just scales the initial magnitudes for the models first predictions. \n",
    "\n",
    "Depending on whether there are convolutions in the network will mean we will need to reshape appropriately. Also note that the audio generated is saved in the audio variable as a 2d numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amount_samples      = 8\n",
    "sequence_length_max = 1500\n",
    "impulse_scale       = 0.1\n",
    "griffin_iterations  = 100\n",
    "\n",
    "dimension1 = dataset.x_frames.shape[1]\n",
    "dimension2 = dataset.x_frames.shape[2]\n",
    "shape = (1, dimension1, dimension2, 1) if use_cnn else (1, dimension1, dimension2)\n",
    "\n",
    "audio = []\n",
    "for i in range(amount_samples):                                                                                                                                   \n",
    "    \n",
    "    random_index = np.random.randint(0, (len(dataset.x_frames) - 1))                                                                                                                    \n",
    "                                                                                                                                                                              \n",
    "    impulse = np.array(dataset.x_frames[random_index]) * impulse_scale\n",
    "    predicted_magnitudes = impulse\n",
    "    \n",
    "    for j in range(sequence_length_max):\n",
    "\n",
    "        prediction = model.predict(impulse.reshape(shape))\n",
    "        \n",
    "        if use_cnn:\n",
    "            prediction = prediction.reshape(1, dataset.y_frames.shape[1], 1)\n",
    "            \n",
    "        predicted_magnitudes = np.vstack((predicted_magnitudes, prediction))                                                                                                  \n",
    "        impulse = predicted_magnitudes[-sequence_length:]\n",
    "        \n",
    "        done = int(float(i * sequence_length_max + j) / float(amount_samples * sequence_length_max) * 100.0) + 1\n",
    "        sys.stdout.write('{}% audio generation complete.   \\r'.format(done))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if done == 100 and notifications_on:\n",
    "            email.send()\n",
    "            notifications_on = False\n",
    "                                                                                                                                                                              \n",
    "    predicted_magnitudes = np.array(predicted_magnitudes).reshape(-1, 513)                                                                           \n",
    "    audio += [dataset.griffin_lim(predicted_magnitudes.T, griffin_iterations)]\n",
    "\n",
    "audio = np.array(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Plotting the Results\n",
    "\n",
    "Select the index you want to listen to. It may be useful to plot stuff below too using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "i = 1\n",
    "Audio(audio[i], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.specgram(audio[i], NFFT=2048, Fs=sample_rate, noverlap=512)\n",
    "\n",
    "# Plot a spectrogram\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testaudio = dataset.griffin_lim(dataset.x_frames[100].T)\n",
    "Audio(testaudio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset.x_frames[5][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset.y_frames[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
