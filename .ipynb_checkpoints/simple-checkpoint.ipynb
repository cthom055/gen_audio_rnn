{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries\n",
    "\n",
    "This makes use of tflearn and tensorflow. Generally speaking, better results were used when using higher level wrappers such as tflearn or slim, so we elected to use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*.npy\r\n",
      "*.meta\r\n",
      "*.index\r\n",
      "*.data*\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import pywt\n",
    "from tflearn.layers.recurrent import bidirectional_rnn, BasicLSTMCell\n",
    "from tflearn.layers.core import dropout\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from audio_dataset_generator import AudioDatasetGenerator\n",
    "from audio_dataset_generator import AudioWaveletDatasetGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Program Parameters\n",
    "\n",
    "Set the appropriate settings for the model here. Audio data path will choose which folder to get fft frames from. Make sure the files are .wav and they are the only files you want as all the files fft frames are loaded and concatenated together. \n",
    "\n",
    "Please note:\n",
    "* ```rnn_type``` should equal any of the following values: lstm, gru, bi_lstm, bi_gru\n",
    "* ```number_rnn_layers``` should be greater than 0\n",
    "* ```activation``` should be the string defined in the tflearn library for [any of the activations defined here](http://tflearn.org/activations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "load_model           = False\n",
    "\n",
    "# Dataset\n",
    "sequence_length      = 40\n",
    "audio_data_path      = \"assets/electronic_piano/\"\n",
    "force_new_dataset    = True\n",
    "use_wavelets         = False\n",
    "predict_coeff        = False\n",
    "wavelet              = 'db10'\n",
    "\n",
    "# Feature Extraction and Audio Genreation\n",
    "sample_rate          = 22050\n",
    "fft_settings         = [2048, 1024, 512]\n",
    "fft_size             = fft_settings[0]\n",
    "window_size          = fft_settings[1]\n",
    "hop_size             = fft_settings[2]\n",
    "\n",
    "# General Network\n",
    "learning_rate        = 1e-3\n",
    "amount_epochs        = 700\n",
    "batch_size           = 64\n",
    "keep_prob            = 0.2\n",
    "loss_type            = \"mean_square\"\n",
    "activation           = 'tanh'\n",
    "optimiser            = 'adam'\n",
    "fully_connected_dim  = 1024\n",
    "\n",
    "# Recurrent Neural Network\n",
    "rnn_type             = \"lstm\"\n",
    "number_rnn_layers    = 3\n",
    "rnn_number_units     = 1024\n",
    "\n",
    "# Convolutional Neural Network\n",
    "use_cnn              = False\n",
    "number_filters       = [32, 64]\n",
    "filter_sizes         = [3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load the Dataset\n",
    "\n",
    "Take the fft magnitudes from the folder specified at the audio_data_path and create the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% data generation complete.  (729, 40, 1025)\n"
     ]
    }
   ],
   "source": [
    "if use_wavelets:\n",
    "    dataset = AudioWaveletDatasetGenerator(1024, sequence_length, \n",
    "                                    sample_rate)\n",
    "else:\n",
    "    dataset = AudioDatasetGenerator(fft_size, window_size, hop_size,\n",
    "                                sequence_length, sample_rate)\n",
    "\n",
    "dataset.load(audio_data_path, force_new_dataset)\n",
    "\n",
    "if use_cnn:\n",
    "    dataset.x_frames = dataset.x_frames.reshape(dataset.x_frames.shape[0], \n",
    "                                                dataset.x_frames.shape[1], \n",
    "                                                dataset.x_frames.shape[2], 1)\n",
    "print(dataset.x_frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "A couple of helper methods are defined to speed up the process of experimenting with the model's archetecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(net, filters, kernels, non_linearity):  \n",
    "    \"\"\"\n",
    "    A quick function to build a conv net. \n",
    "    At the end it reshapes the network to be 3d to work with recurrent units.\n",
    "    \"\"\"\n",
    "    assert len(filters) == len(kernels)\n",
    "    \n",
    "    for i in range(len(filters)):\n",
    "        net = conv_2d(net, filters[i], kernels[i], activation=non_linearity)\n",
    "        net = max_pool_2d(net, 2)\n",
    "        \n",
    "    dim1 = net.get_shape().as_list()[1]\n",
    "    dim2 = net.get_shape().as_list()[2]\n",
    "    dim3 = net.get_shape().as_list()[3]\n",
    "    return tf.reshape(net, [-1, dim1 * dim3, dim2])\n",
    "   \n",
    "                      \n",
    "def recurrent_net(net, rec_type, rec_size, return_sequence):\n",
    "    \"\"\"\n",
    "    A quick if else block to build a recurrent layer, based on the type specified\n",
    "    by the user.\n",
    "    \"\"\"\n",
    "    if rec_type == 'lstm':\n",
    "        net = tflearn.layers.recurrent.lstm(net, rec_size, return_seq=return_sequence)\n",
    "    elif rec_type == 'gru':\n",
    "        net = tflearn.layers.recurrent.gru(net, rec_size, return_seq=return_sequence)\n",
    "    elif rec_type == 'bi_lstm':\n",
    "        net = bidirectional_rnn(net, \n",
    "                                BasicLSTMCell(rec_size), \n",
    "                                BasicLSTMCell(rec_size), \n",
    "                                return_seq=return_sequence)\n",
    "    elif rec_type == 'bi_gru':\n",
    "        net = bidirectional_rnn(net, \n",
    "                                GRUCell(rec_size), \n",
    "                                GRUCell(rec_size), \n",
    "                                return_seq=return_sequence)\n",
    "    else:\n",
    "        raise ValueError('Incorrect rnn type passed. Try lstm, gru, bi_lstm or bi_gru.')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the actual structure of the net is specified.\n",
    "\n",
    "If use_cnn is true the model will be prefixed with a cnn. This will slow things down but produces a different kind of result. The model regardless then builds a rnn layer which runs into a fully connected layer. before being passed to linear outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "if use_cnn:\n",
    "    net = tflearn.input_data([None, \n",
    "                              dataset.x_frames.shape[1], \n",
    "                              dataset.x_frames.shape[2], \n",
    "                              dataset.x_frames.shape[3]], name=\"input_data0\")\n",
    "    net = conv_net(net, number_filters, filter_sizes, activation)\n",
    "else:                  \n",
    "    net = tflearn.input_data([None, \n",
    "                              dataset.x_frames.shape[1], \n",
    "                              dataset.x_frames.shape[2]], name=\"input_data0\") \n",
    "\n",
    "# Batch Norm\n",
    "net = tflearn.batch_normalization(net, name=\"batch_norm0\")\n",
    "  \n",
    "# Recurrent\n",
    "for layer in range(number_rnn_layers):\n",
    "    return_sequence = False if layer == (number_rnn_layers - 1) else True\n",
    "    net = recurrent_net(net, rnn_type, rnn_number_units, return_sequence)\n",
    "    net = dropout(net, 1-keep_prob) if keep_prob < 1.0 else net \n",
    "\n",
    "# Dense + MLP Out\n",
    "net = tflearn.fully_connected(net, dataset.y_frames.shape[1], \n",
    "                              activation=activation,                                            \n",
    "                              regularizer='L2', \n",
    "                              weight_decay=0.001)\n",
    "                      \n",
    "net = tflearn.fully_connected(net, dataset.y_frames.shape[1], \n",
    "                              activation='linear')\n",
    "\n",
    "net = tflearn.regression(net, optimizer=optimiser, learning_rate=learning_rate,                                 \n",
    "                         loss=loss_type)\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=1,checkpoint_path=\"models/\")\n",
    "#model = tflearn.models.generator.SequenceGenerator(net, tensorboard_verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Currently we aren't worried about overfitting, so we just pass the entire dataset of generated magnitudes. Perhaps we might want to change this, in which case we would need to split the dataset into training, validation and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.39740\u001b[0m\u001b[0m | time: 8.856s\n",
      "| Adam | epoch: 028 | loss: 0.39740 - acc: 0.2860 -- iter: 704/729\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.39431\u001b[0m\u001b[0m | time: 9.650s\n",
      "| Adam | epoch: 028 | loss: 0.39431 - acc: 0.2918 -- iter: 729/729\n",
      "--\n",
      "INFO:tensorflow:/notebooks/gen_audio_rnn/models-336 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/Dropout.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-33f688a95430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(dataset.x_frames, dataset.y_frames, show_metric=True, \n\u001b[0;32m----> 2\u001b[0;31m           batch_size=batch_size, n_epoch=amount_epochs)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 818\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(dataset.x_frames, dataset.y_frames, show_metric=True, \n",
    "          batch_size=batch_size, n_epoch=amount_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model.tfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load(\"model.tfl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Audio\n",
    "\n",
    "Here we generate audio. Choose the amount of samples, then how long they should be with sequence length, and then how many iterations the griffin lim algorithm should run for. The impluse scale is something we haven't objectively tested yet, but it just scales the initial magnitudes for the models first predictions. \n",
    "\n",
    "Depending on whether there are convolutions in the network will mean we will need to reshape appropriately. Also note that the audio generated is saved in the audio variable as a 2d numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amount_samples      = 1\n",
    "sequence_length_max = 1000\n",
    "impulse_scale       = 1.0\n",
    "griffin_iterations  = 60\n",
    "random_chance       = 0.1\n",
    "random_strength     = 0.3\n",
    "\n",
    "dimension1 = dataset.x_frames.shape[1]\n",
    "dimension2 = dataset.x_frames.shape[2]\n",
    "shape = (1, dimension1, dimension2, 1) if use_cnn else (1, dimension1, dimension2)\n",
    "\n",
    "audio = []\n",
    "\n",
    "if use_wavelets:\n",
    "    temp_audio = np.array(0)\n",
    "for i in range(amount_samples):                                                                                                                                   \n",
    "    \n",
    "    random_index = np.random.randint(0, (len(dataset.x_frames) - 1))                                                                                                                    \n",
    "                                                                                                                                                                              \n",
    "    impulse = np.array(dataset.x_frames[random_index]) * impulse_scale\n",
    "    predicted_magnitudes = impulse\n",
    "    \n",
    "    if use_wavelets:\n",
    "        for seq in range (impulse.shape[0]):\n",
    "            coeffs = pywt.array_to_coeffs(impulse[seq], dataset.coeff_slices)\n",
    "            recon = (pywt.waverecn(coeffs, wavelet=wavelet))\n",
    "            temp_audio = np.append(temp_audio, recon)\n",
    "    for j in range(sequence_length_max):\n",
    "\n",
    "        prediction = model.predict(impulse.reshape(shape))\n",
    "        \n",
    "        #Wavelet audio\n",
    "        if use_wavelets:\n",
    "            coeffs = pywt.array_to_coeffs(prediction[0], dataset.coeff_slices)\n",
    "            recon = (pywt.waverecn(coeffs, wavelet=wavelet))\n",
    "            temp_audio = np.append(temp_audio, recon)\n",
    "        \n",
    "        if use_cnn:\n",
    "            prediction = prediction.reshape(1, dataset.y_frames.shape[1], 1)\n",
    "        \n",
    "        predicted_magnitudes = np.vstack((predicted_magnitudes, prediction))                                                                                                  \n",
    "        impulse = predicted_magnitudes[-sequence_length:]\n",
    "        \n",
    "        if (np.random.random_sample() < random_chance) :\n",
    "            idx = np.random.randint(0, dataset.sequence_length)\n",
    "            impulse[idx] = impulse[idx] + np.random.random_sample(impulse[idx].shape) * random_strength\n",
    "        \n",
    "        done = int(float(i * sequence_length_max + j) / float(amount_samples * sequence_length_max) * 100.0) + 1\n",
    "        sys.stdout.write('{}% audio generation complete.   \\r'.format(done))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if use_wavelets:                                                                                                                                                                        \n",
    "        audio += [temp_audio]\n",
    "    else:\n",
    "        predicted_magnitudes = np.array(predicted_magnitudes).reshape(-1, window_size+1)                                                                           \n",
    "        audio += [dataset.griffin_lim(predicted_magnitudes.T, griffin_iterations)]\n",
    "audio = np.array(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Plotting the Results\n",
    "\n",
    "Select the index you want to listen to. It may be useful to plot stuff below too using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "i = 0\n",
    "Audio(audio[i], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.specgram(audio[i], NFFT=2048, Fs=sample_rate, noverlap=512)\n",
    "\n",
    "# Plot a spectrogram\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just to test the input sound\n",
    "testaudio = dataset.griffin_lim(dataset.x_frames[10].T)\n",
    "Audio(testaudio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset.x_frames[5][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset.y_frames[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
